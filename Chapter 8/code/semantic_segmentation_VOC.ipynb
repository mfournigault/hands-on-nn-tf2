{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important notes\n",
    "**The following code proposed in the book is not executable, at least on google colab.**\n",
    "It requires to install the package tensorflow_datasets in version 2.1.0 with tensorflow 2.7.\n",
    "Tensorflow 2.7 is no longer available on Google colab, the oldest version available is tensorflow 2.12.\n",
    "Which is not compatible with the usage of the class tdfs.image.Voc2007, for the task of segmentation as proposed in the book.\n",
    "(Because tdfs in v2.1.0 requires the installation of protobuf in v3.2.0 not compatible with tensorflow 2.12)\n",
    "\n",
    "The problem is that with newer versions of TDFS, the datasets VOC are no longer available for segmentation tasks.\n",
    "\n",
    "So the execution of the following code for this task and this dataset, would require important changes to download and use segmentation labels available in the raw VOC datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(depth):\n",
    "    return tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Conv2D(\n",
    "                depth, 3, strides=2, padding=\"same\", kernel_initializer=\"he_normal\"\n",
    "            ),\n",
    "            tf.keras.layers.LeakyReLU(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def upsample(depth):\n",
    "    return tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Conv2DTranspose(\n",
    "                depth, 3, strides=2, padding=\"same\", kernel_initializer=\"he_normal\"\n",
    "            ),\n",
    "            tf.keras.layers.ReLU(),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet(input_size=(256, 256, 3), num_classes=21):\n",
    "\n",
    "    # Downsample from 256x256 to 4x4, while adding depth\n",
    "    # using powers of 2, startin from 2**5. Cap to 512.\n",
    "    encoders = []\n",
    "    for i in range(2, int(math.log2(256))):\n",
    "        depth = 2 ** (i + 5)\n",
    "        if depth > 512:\n",
    "            depth = 512\n",
    "        encoders.append(downsample(depth=depth))\n",
    "\n",
    "    # Upsample from 4x4 to 256x256, reducing the depth\n",
    "    decoders = []\n",
    "    for i in reversed(range(2, int(math.log2(256)))):\n",
    "        depth = 2 ** (i + 5)\n",
    "        if depth < 32:\n",
    "            depth = 32\n",
    "        if depth > 512:\n",
    "            depth = 512\n",
    "        decoders.append(upsample(depth=depth))\n",
    "\n",
    "    # Build the model by invoking the encoder layers with the correct input\n",
    "    inputs = tf.keras.layers.Input(input_size)\n",
    "    concat = tf.keras.layers.Concatenate()\n",
    "\n",
    "    x = inputs\n",
    "    # Encoder: downsample loop\n",
    "    skips = []\n",
    "    for conv in encoders:\n",
    "        x = conv(x)\n",
    "        skips.append(x)\n",
    "\n",
    "    skips = reversed(skips[:-1])\n",
    "\n",
    "    # Decoder: input + skip connection\n",
    "    for deconv, skip in zip(decoders, skips):\n",
    "        x = deconv(x)\n",
    "        x = tf.keras.layers.Concatenate()([x, skip])\n",
    "\n",
    "    # Add the last layer on top and define the model\n",
    "    last = tf.keras.layers.Conv2DTranspose(\n",
    "        num_classes, 3, strides=2, padding=\"same\", kernel_initializer=\"he_normal\"\n",
    "    )\n",
    "\n",
    "    outputs = last(x)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LUT = {\n",
    "    (0, 0, 0): 0, # background\n",
    "    (128, 0, 0): 1, # aeroplane\n",
    "    (0, 128, 0): 2, # bicycle\n",
    "    (128, 128, 0): 3, # bird\n",
    "    (0, 0, 128): 4, # boat\n",
    "    (128, 0, 128): 5, # bottle\n",
    "    (0, 128, 128): 6, # bus\n",
    "    (128, 128, 128): 7, # car\n",
    "    (64, 0, 0): 8, # cat\n",
    "    (192, 0, 0): 9, # chair\n",
    "    (64, 128, 0): 10, # cow\n",
    "    (192, 128, 0): 11, # diningtable\n",
    "    (64, 0, 128): 12, # dog\n",
    "    (192, 0, 128): 13, # horse\n",
    "    (64, 128, 128): 14, # motorbike\n",
    "    (192, 128, 128): 15, # person\n",
    "    (0, 64, 0): 16, # pottedplant\n",
    "    (128, 64, 0): 17, # sheep\n",
    "    (0, 192, 0): 18, # sofa\n",
    "    (128, 192, 0): 19, # train\n",
    "    (0, 64, 128): 20, # tvmonitor\n",
    "    (255, 255, 255): 21, # undefined / don't care\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Voc2007Semantic(tfds.image.Voc2007): \n",
    "    \"\"\"Pasval VOC 2007 - semantic segmentation.\"\"\" \n",
    " \n",
    "    VERSION = tfds.core.Version(\"0.1.0\")\n",
    "    def _info(self):\n",
    "        parent_info = tfds.image.Voc2007().info\n",
    "        return tfds.core.DatasetInfo(\n",
    "            builder=self,\n",
    "            description=parent_info.description,\n",
    "            features=tfds.features.FeaturesDict(\n",
    "                {\n",
    "                    \"image\": tfds.features.Image(shape=(None, None, 3)),\n",
    "                    \"image/filename\": tfds.features.Text(),\n",
    "                    \"label\": tfds.features.Image(shape=(None, None, 1)),\n",
    "                }\n",
    "            ),\n",
    "            urls=parent_info.urls,\n",
    "            citation=parent_info.citation,\n",
    "        )\n",
    "\n",
    "      \n",
    "    def _generate_examples(self, data_path, set_name):\n",
    "        set_filepath = os.path.join(\n",
    "            data_path,\n",
    "            \"VOCdevkit/VOC2007/ImageSets/Segmentation/{}.txt\".format(set_name),\n",
    "        )\n",
    "        with tf.io.gfile.GFile(set_filepath, \"r\") as f:\n",
    "            for line in f:\n",
    "                image_id = line.strip()\n",
    "\n",
    "                image_filepath = os.path.join(\n",
    "                    data_path, \"VOCdevkit\", \"VOC2007\", \"JPEGImages\", f\"{image_id}.jpg\"\n",
    "                )\n",
    "                label_filepath = os.path.join(\n",
    "                    data_path,\n",
    "                    \"VOCdevkit\",\n",
    "                    \"VOC2007\",\n",
    "                    \"SegmentationClass\",\n",
    "                    f\"{image_id}.png\",\n",
    "                )\n",
    "\n",
    "                if not tf.io.gfile.exists(label_filepath):\n",
    "                    continue\n",
    "\n",
    "                label_rgb = tf.image.decode_image(\n",
    "                    tf.io.read_file(label_filepath), channels=3\n",
    "                )\n",
    "\n",
    "                label = tf.Variable(\n",
    "                    tf.expand_dims(\n",
    "                        tf.zeros(shape=tf.shape(label_rgb)[:-1], dtype=tf.uint8), -1\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                for color, label_id in LUT.items():\n",
    "                    match = tf.reduce_all(tf.equal(label_rgb, color), axis=[2])\n",
    "                    labeled = tf.expand_dims(tf.cast(match, tf.uint8), axis=-1)\n",
    "                    label.assign_add(labeled * label_id)\n",
    "\n",
    "                colored = tf.not_equal(tf.reduce_sum(label), tf.constant(0, tf.uint8))\n",
    "                # Certain labels have wrong RGB values\n",
    "                if not colored.numpy():\n",
    "                    tf.print(\"error parsing: \", label_filepath)\n",
    "                    continue\n",
    "                \n",
    "                yield {\n",
    "                    # Declaring in _info \"image\" as a tfds.feature.Image\n",
    "                    # we can use both an image or a string. If a string is detected\n",
    "                    # it is supposed to be the image path and tfds take care of the\n",
    "                    # reading process.\n",
    "                    \"image\": image_filepath,\n",
    "                    \"image/filename\": f\"{image_id}.jpg\",\n",
    "                    \"label\": label.numpy(),\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfds.list_builders())\n",
    "dataset, info = tfds.load(\"voc2007_semantic\", with_info=True)\n",
    "\n",
    "train_set = dataset[\"train\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_scale(row):\n",
    "    # Resize and convert to float, [0,1] range\n",
    "    row[\"image\"] = tf.image.convert_image_dtype(\n",
    "        tf.image.resize(\n",
    "            row[\"image\"],\n",
    "            (256,256),\n",
    "            method=tf.image.ResizeMethod.NEAREST_NEIGHBOR),\n",
    "        tf.float32)\n",
    "    # Resize, cast to int64 since it is a supported label type\n",
    "    row[\"label\"] = tf.cast(\n",
    "        tf.image.resize(\n",
    "            row[\"label\"],\n",
    "            (256,256),\n",
    "            method=tf.image.ResizeMethod.NEAREST_NEIGHBOR),\n",
    "        tf.int64)\n",
    "    return row\n",
    "  \n",
    "def to_pair(row):\n",
    "    return row[\"image\"], row[\"label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 32\n",
    "\n",
    "train_set = train_set.map(resize_and_scale).map(to_pair)\n",
    "train_set = train_set.batch(batch_size).prefetch(1)\n",
    "\n",
    "validation_set = dataset[\"validation\"].map(resize_and_scale)\n",
    "validation_set = validation_set.map(to_pair).batch(batch_size)\n",
    "\n",
    "model = get_unet()\n",
    "\n",
    "optimizer = tf.optimizers.Adam()\n",
    "\n",
    "checkpoint_path = \"ckpt/pb.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create checkpoint callback\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(write_images=True)\n",
    "model.compile(optimizer=optimizer,\n",
    "              #loss=lambda y_true, y_pred: tf.losses.SparseCategoricalCrossentropy(from_logits=True)(y_true, y_pred) + tf.losses.MeanAbsoluteError()(y_true, y_pred),\n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])#, tf.metrics.MeanIoU(num_classes=21)])\n",
    "\n",
    "\n",
    "num_epochs = 50\n",
    "model.fit(train_set, validation_data=validation_set, epochs=num_epochs,\n",
    "          callbacks=[cp_callback, tensorboard])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = tf.image.decode_jpeg(tf.io.read_file(\"me.jpg\"))\n",
    "sample = tf.expand_dims(tf.image.convert_image_dtype(sample, tf.float32), axis=[0])\n",
    "sample = tf.image.resize(sample, (512,512))\n",
    "pred_image = tf.squeeze(tf.argmax(model(sample), axis=-1), axis=[0])\n",
    "\n",
    "REV_LUT = {value: key for key, value in LUT.items()}\n",
    "\n",
    "color_image = tf.Variable(tf.zeros((512,512,3), dtype=tf.uint8))\n",
    "pixels_per_label = []\n",
    "for label, color in REV_LUT.items():\n",
    "    match = tf.equal(pred_image, label)\n",
    "    labeled = tf.expand_dims(tf.cast(match, tf.uint8), axis=-1)\n",
    "    pixels_per_label.append((label, tf.math.count_nonzero(labeled)))\n",
    "    labeled = tf.tile(labeled, [1,1,3])\n",
    "    color_image.assign_add(labeled * color)\n",
    "\n",
    "\n",
    "for label, count in pixels_per_label:\n",
    "    print(label, \": \", count.numpy())\n",
    "\n",
    "tf.io.write_file(\"seg.jpg\", tf.io.encode_jpeg(color_image))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
